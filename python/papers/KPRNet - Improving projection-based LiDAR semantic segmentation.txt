KPRNet: Improving projection-based LiDAR
semantic segmentation
Deyvid Kochanov, Fatemeh Karimi Nejadasl, and Olaf Booij
TomTom, Amsterdam, Netherlands
{deyvid.kochanov,fatemeh.kariminejadasl,olaf.booij}@tomtom.com
Abstract. Semantic segmentation is an important component in the
perception systems of autonomous vehicles. In this work, we adopt re-
cent advances in both image and point cloud segmentation to achieve
a better accuracy in the task of segmenting LiDAR scans. KPRNet im-
proves the convolutional neural network architecture of 2D projection
methods and utilizes KPConv [11] to replace the commonly used post-
processing techniques with a learnable point-wise component which al-
lows us to obtain more accurate 3D labels. With these improvements
our model outperforms the current best method on the SemanticKITTI
benchmark, reaching an mIoU of 63.1.
Keywords: LiDAR, point clouds, semantic segmentation
1 Introduction
Semantic segmentation plays an important role in the autonomous driving per-
ception stack. This started mainly with the segmentation of camera-based im-
ages, but since the release of the SemanticKITTI dataset [1] significant progress
has been made in the segmentation of LiDAR measurements.
Current LiDAR based semantic segmentation can roughly be categorized in
two approaches, which reach a comparable performance. The first approach,
uses purely point-wise methods acting directly on the 3D point cloud [11,6]. The
second approach builds on top of the well developed field of image segmentation,
which focuses on CNN architectures for segmenting RGB images [15,2,14,10]. To
this end, individual LiDAR sweeps are projected to 2D range images, which then
serve as input to custom CNNs [9,5,13]. The resulting 2D predictions are then
post-processed with non-learned CRFs or KNN-based voting to recover more
accurate labels for each 3D point.
In this work, we combine the best of both these approaches. The main contri-
bution is twofold. First, we propose an improved CNN architecture for 2D pro-
jected LiDAR sweeps. Next, we replace the post-processing step with a learnable
module based on KPConv [11].
On the SemanticKITTI benchmark the resulting model outperforms the cur-
rent best 2D segmentation method SalsaNext [5] by 3.6 mIoU points and the
best point-wise method which relies on KPConv by 4.3 mIoU points.1
1 20-July-2020 submitted to CodaLab. The name is not bad jpg.
ar
X
iv
:2
00
7.
12
66
8v
1 
 [
cs
.C
V
] 
 2
4 
Ju
l 
20
20
2 Kochanov et al.
2 Method
The proposed method combines a 2D semantic segmentation network (Sec-
tion 2.1), and a 3D point-wise layer. The convolutional network gets as input a
LiDAR scan projected to a range image. The resulting 2D CNN features are pro-
jected back to their respective 3D points and passed to a 3D point-wise module
(Section 2.2), which predicts the final labels.
Fig. 1. The KPRNet architecture: ResNeXt features with stride 16 are fed into an
ASPP module [2] and combined with the outputs of the second and first ResNeXt
blocks, which have strides of 8 and 4. The result is passed through a KPConv layer
followed by BatchNorm, ReLu and a final classifier.
2.1 2D semantic segmentation
In our model, we use a ResNeXt-101 encoder [8] and a decoder similar to
Panoptic-DeepLab [3] (see Figure 1). The CNN part of our final architecture
is pre-trained on Cityscapes [4]. When transferring the model to the LiDAR
segmentation task, we discard one of the filter planes in the first layer because
the input for the task requires only 2 channels - inverse depth and reflectivity.
2.2 3D semantic segmentation
To generate 2D range images most methods [9,5,13] perform a spherical pro-
jection on the point cloud. In [12] an alternative method was proposed which
unfolds the scans according to the order in which they are captured by the
LiDAR sensor. This results in smoother projections and we use it in all our
experiments.
Despite this, the remaining discretization artifacts and overly-smooth 2D
labels generated by the CNN result in misprediction when back-projecting to the
3D point cloud. RangeNet++ [9] and other 2D segmentation methods perform a
KNN or CRF post-processing step to account for this but finding a good balance
between over- and under-smoothing the 3D labels with these methods can be
difficult.
KPRNet 3
We replace this post-processing step by inserting a single KPConv [11] layer
before the final classification (see Figure 1). KPConv is a point-convolution op-
erator which can learn to correct the misclassifications by taking into account
the 2D features of each point and its surrounding 3D context. The required mod-
ification to the CNN architecture is minimal and the pipeline from a 2D range
image to 3D point labels is end-to-end learnable.
3 Experiments
3.1 Setting
We train and evaluate our model on SemanticKITTI which contains 21 se-
quences. Sequences 11-21 are reserved as a test split and only available as the
official benchmark. Sequence 8 is used for validation and the rest is used for
training.
All the models are trained with SGD with momentum of 0.9 and 1e-4 weight
decay for 120 epochs. We use a cosine schedule [7] with warm-up in the first
1000 iterations. For the KNN experiments we used a batch size of 32 and a base
learning rate of 0.025. The KPConv models are trained with a batch size of 24
and the base learning rate is set to 0.01875. During training random crops of
width 1025 are sampled from the range images and random horizontal flipping
is applied. All models are trained on 8 Tesla V100-SXM2-16GB GPUs.
3.2 Quantitative evaluation
Approach c
a
r
b
ic
y
c
le
m
o
to
rc
y
c
le
tr
u
ck
o
th
e
r-
v
e
h
ic
le
p
e
rs
o
n
b
ic
y
c
li
st
m
o
to
rc
y
c
li
st
ro
a
d
p
a
rk
in
g
si
d
e
w
a
lk
o
th
e
r-
g
ro
u
n
d
b
u
il
d
in
g
fe
n
c
e
v
e
g
e
ta
ti
o
n
tr
u
n
k
te
rr
a
in
p
o
le
tr
a
ffi
c
-s
ig
n
m
e
a
n
-I
o
U
SalsaNext [5] 91.9 48.3 38.6 38.9 31.9 60.2 59.0 19.4 91.7 63.7 75.8 29.1 90.2 64.2 81.8 63.6 66.5 54.3 62.1 59.5
KPConv [11] 96.0 30.2 42.5 33.4 44.3 61.5 61.6 11.8 88.8 61.3 72.7 31.6 90.5 64.2 84.8 69.2 69.1 56.4 47.4 58.8
SqueezeSegV3 [13] 92.5 38.7 36.5 29.6 33.0 45.6 46.2 20.1 91.7 63.4 74.8 26.4 89.0 59.4 82.0 58.7 65.4 49.6 58.9 55.9
RandLa-Net [6] 94.2 26.0 25.8 40.1 38.9 49.2 48.2 7.2 90.7 60.3 73.7 20.4 86.9 56.3 81.4 61.3 66.8 49.2 47.7 53.9
RangeNet++ [9] 91.4 25.7 34.4 25.7 23.0 38.3 38.8 4.8 91.8 65.0 75.2 27.8 87.4 58.6 80.5 55.1 64.6 47.9 55.9 52.2
KPRNet [Ours] 95.5 54.1 47.9 23.6 42.6 65.9 65.0 16.5 93.2 73.9 80.6 30.2 91.7 68.4 85.7 69.8 71.2 58.7 64.1 63.1
Table 1. Quantitative comparison on SemanticKITTI test benchmark.
First we report results on the validation set. We train our CNN architecture
and use the KNN post-processing [9] as a baseline.
The input scans have relatively low vertical resolution of 64x2048. To ac-
count for this RangeNet++ uses only horizontal strides in their down-sampling
layers. Instead of that, we simply upscale the input range image to 145x2049
using nearest neighbour interpolation. This baseline model achieves 57.7 mIoU.
In our second experiment we further upsample the image to 289x4097. This
4 Kochanov et al.
model achieves 61.7 mIoU on the validation set. Next we replace the KNN post-
processing with a KPConv layer which results in 64.1 mIoU.
We train this final model on the combined training and validation subsets
and submit to the official SemanticKITTI benchmark. The evaluation results
and comparison to state-of-the-art models are show in Table 1.
References
1. Behley, J., Garbade, M., Milioto, A., Quenzel, J., Behnke, S., Stachniss, C., Gall, J.:
SemanticKITTI: A dataset for semantic scene understanding of LiDAR sequences.
In: ICCV (2019)
2. Chen, L.C., Papandreou, G., Schroff, F., Adam, H.: Rethinking atrous convolution
for semantic image segmentation. arXiv preprint arXiv:1706.05587 (2017)
3. Cheng, B., Collins, M.D., Zhu, Y., Liu, T., Huang, T.S., Adam, H., Chen, L.C.:
Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic seg-
mentation. In: CVPR. pp. 12475–12485 (2020)
4. Cordts, M., Omran, M., Ramos, S., Rehfeld, T., Enzweiler, M., Benenson, R.,
Franke, U., Roth, S., Schiele, B.: The Cityscapes dataset for semantic urban scene
understanding. In: CVPR (2016)
5. Cortinhal, T., Tzelepis, G., Aksoy, E.E.: SalsaNext: Fast, uncertainty-aware se-
mantic segmentationof LiDAR point clouds for autonomous driving. Arxiv (2020)
6. Hu, Q., Markham, A., Rosa, S., Trigoni, N., Wang, Z., Xie, L., Yang, B.: Randla-
net: Efficient semantic segmentation of large- scale point clouds. In: CVPR (2020)
7. Loshchilov, I., Hutter, F.: SGDR: Stochastic gradient descent with warm restarts.
In: ICLR (2017)
8. Mahajan, D., Girshick, R., Ramanathan, V., He, K., Paluri, M., Li, Y., Bharambe,
A., van der Maaten, L.: Exploring the limits of weakly supervised pretraining. In:
ECCV. pp. 181–196 (2018)
9. Milioto, A., Vizzo, I., Behley, J., Stachniss, C.: RangeNet++: Fast and accurate
LiDAR semantic segmentation. In: IROS. pp. 4213–4220. IEEE (2019)
10. Sun, K., Zhao, Y., Jiang, B., Cheng, T., Xiao, B., Liu, D., Mu, Y., Wang, X.,
Liu, W., Wang, J.: High-resolution representations for labeling pixels and regions.
arXiv:1904.04514 (2019)
11. Thomas, H., Qi, C.R., Deschaud, J.E., Marcotegui, B., Goulette, F., Guibas, L.J.:
KPConv: Flexible and deformable convolution for point clouds. In: ICCV (2019)
12. Triess, L.T., Peter, D., Rist, C.B., Zöllner, J.M.: Scan-based semantic segmenta-
tion of lidar point clouds: An experimental study. arXiv preprint arXiv:2004.11803
(2020)
13. Xu, C., Wu, B., Wang, Z., Zhan, W., Vajda, P., Keutzer, K., Tomizuka, M.:
SqueezeSegV3: Spatially-adaptive convolution for efficient point-cloud segmenta-
tion. arXiv preprint arXiv:2004.01803 (2020)
14. Yuan, Y., Chen, X., Wang, J.: Object-contextual representations for semantic seg-
mentation. arXiv preprint arXiv:1909.11065 (2019)
15. Zhao, H., Shi, J., Qi, X., Wang, X., Jia, J.: Pyramid scene parsing network. In:
CVPR. pp. 2881–2890 (2017)
	KPRNet: Improving projection-based LiDAR semantic segmentation