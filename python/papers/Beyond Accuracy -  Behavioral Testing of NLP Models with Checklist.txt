{ "Paper": {
    "id": "1",
    "title": "Beyond Accuracy: Behavioral Testing of NLP Models with CheckList",
    "authors": ["Marco Tulio Ribeiro", "Tongshuang Wu", "Carlos Guestrin", "Sameer Singh"],
    "content": [
            {
                "sectionTitle": "Abstract",
                "text": "Although measuring held-out accuracy has been the primary approach to evaluate generalization, it often overestimates the performance of NLP models, while alternative approaches for evaluating models either focus on individual tasks or on specific behaviors. Inspired by principles of behavioral testing in software engineering, we introduce CheckList, a task-agnostic methodology for testing NLP models. CheckList includes a matrix of general linguistic capabilities and test types that facilitate comprehensive test ideation, as well as a software tool to generate a large and diverse number of test cases quickly. We illustrate the utility of CheckList with tests for three tasks, identifying critical failures in both commercial and state-of-art models. In a user study, a team responsible for a commercial sentiment analysis model found new and actionable bugs in an extensively tested model. In another user study, NLP practitioners with CheckList created twice as many tests, and found almost three times as many bugs as users without it."
            },
            {
                "sectionTitle": "1 Introduction",
                "text": "One of the primary goals of training NLP models is generalization. Since testing “in the wild” is expensive and does not allow for fast iterations, the standard paradigm for evaluation is using train- validation-test splits to estimate the accuracy of the model, including the use of leader boards to track progress on a task (Rajpurkar et al., 2016). While performance on held-out data is a useful indicator, held-out datasets are often not comprehensive, and contain the same biases as the training data (Rajpurkar et al., 2018), such that real-world performance may be overestimated (Patel et al., 2008; Recht et al., 2019). Further, by summarizing the performance as a single aggregate statistic, it becomes difficult to figure out where the model is failing, and how to fix it (Wu et al., 2019). A number of additional evaluation approaches have been proposed, such as evaluating robustness to noise (Belinkov and Bisk, 2018; Rychalska et al., 2019) or adversarial changes (Ribeiro et al., 2018; Iyyer et al., 2018)."
            },
            {
                "sectionTitle": "2 Checklist",
                "text": "Conceptually, users “CheckList” a model by fill- ing out cells in a matrix (Figure 1), each cell po- tentially containing multiple tests. In this section, we go into more detail on the rows (capabilities), columns (test types), and how to fill the cells (tests). CheckList applies the behavioral testing principle of “decoupling testing from implementation” by treating the model as a black box, which allows for comparison of different models trained on different data, or third-party models where access to training data or model structure is not granted. 2.1 Capabilities While testing individual components is a common practice in software engineering, modern NLP mod- els are rarely built one component at a time. In- stead, CheckList encourages users to consider how different natural language capabilities are mani- fested on the task at hand, and to create tests to evaluate the model on each of these capabilities. For example, the Vocabulary+POS capability per- tains to whether a model has the necessary vocab- ulary."
            },
            {
                "sectionTitle": "3 Testing SOTA models with CheckList",
                "text": "We CheckList the following commercial Sentiment analysis models via their paid APIs2: Microsoft’s Text Analytics (q), Google Cloud’s Natural Lan- guage (), and Amazon’s Comprehend (À). We also CheckList BERT-base ( ) and RoBERTa- base (RoB) (Liu et al., 2019) finetuned on SST-23 (acc: 92.7% and 94.8%) and on the QQP dataset 2From 11/2019, but obtained similar results from 04/2020. 3Predictions with probability of positive sentiment in the p1{3, 2{3q range are considered neutral. https://github.com/marcotcr/checklist https://github.com/marcotcr/checklist Label: duplicate =, or non-duplicate ,; INV: same pred. (INV) after removals/ additions Test TYPE and Description Failure Rate Example Test cases & expected behavior RoB Vocab. MFT: Modifiers changes question intent 78.4 78.0 { Is Mark Wright a photographer? | Is Mark Wright an accredited photographer? } , Ta xo no m y MFT: Synonyms in simple templates 22.8 39.2 { How can I become more vocal? | How can I become more outspoken? } = INV: Replace words with synonyms in real pairs 13.1 12.7 Is it necessary to follow a religion?"
            },
            {
                "sectionTitle": "4 User Evaluation",
                "text": "The failures discovered in the previous section demonstrate the usefulness and flexibility of Check- List. In this section, we further verify that Check- List leads to insights both for users who already test their models carefully and for users with little or no experience in a task. 4.1 CheckListing a Commercial System We approached the team responsible for the gen- eral purpose sentiment analysis model sold as a service by Microsoft (q on Table 1). Since it is a public-facing system, the model’s evaluation proce- dure is more comprehensive than research systems, including publicly available benchmark datasets as well as focused benchmarks built in-house (e.g. negations, emojis). Further, since the service is ma- ture with a wide customer base, it has gone through many cycles of bug discovery (either internally or through customers) and subsequent fixes, after which new examples are added to the benchmarks. Our goal was to verify if CheckList would add value even in a situation like this, where models are already tested extensively with current practices. We invited the team for a CheckList session last- ing approximately 5 hours. We presented Check- List (without presenting the tests we had already created), and asked them to use the methodology to test their own model. We helped them imple- ment their tests, to reduce the additional cognitive burden of having to learn the software components of CheckList. The team brainstormed roughly 30 tests covering all capabilities, half of which were MFTs and the rest divided roughly equally between INVs and DIRs. Due to time constraints, we imple- mented about 20 of those tests. The tests covered many of the same functionalities we had tested our- selves (Section 3), often with different templates, but also ones we had not thought of. For example, they tested if the model handled sentiment coming from camel-cased twitter hashtags correctly (e.g. “#IHateYou”, “#ILoveYou”), implicit negation (e.g. “I wish it was good”), and others. Further, they proposed new capabilities for testing, e.g. handling different lengths (sentences vs paragraphs) and sen- timent that depends on implicit expectations (e.g. “There was no {AC}” when {AC} is expected). Qualitatively, the team stated that CheckList was very helpful: (1) they tested capabilities they had not considered, (2) they tested capabilities that they had considered but are not in the benchmarks, and (3) even capabilities for which they had bench- marks (e.g. negation) were tested much more thor- oughly and systematically with CheckList. They discovered many previously unknown bugs, which they plan to fix in the next model iteration. Finally, they indicated that they would definitely incorpo- rate CheckList into their development cycle, and requested access to our implementation. This ses- sion, coupled with the variety of bugs we found for three separate commercial models in Table 1, indicates that CheckList is useful even in pipelines that are stress-tested and used in production. 4.2 User Study: CheckListMFTs We conduct a user study to further evaluate dif- ferent subsets of CheckList in a more controlled environment, and to verify if even users with no previous experience in a task can gain insights and find bugs in a model. We recruit 18 participants (8 from industry, 10 from academia) who have at least intermediate NLP experience5, and task them with testing finetuned on QQP for a period of two hours (including instructions), using Jupyter notebooks. Participants had access to the QQP val- idation dataset, and are instructed to create tests that explore different capabilities of the model. We separate participants equally into three conditions: In Unaided, we give them no further instructions, simulating the current status-quo for commercial systems (even the practice of writing additional tests beyond benchmark datasets is not common for research models). In Cap. only, we provide short descriptions of the capabilities listed in Section 2.1 as suggestions to test, while in Cap.+templ. we further provide them with the template and fill-in tools described in Section 2.3. Only one partici- pant (in Unaided) had prior experience with QQP. Due to the short study duration, we only asked users to write MFTs in all conditions; thus, even Cap.+templ. is a subset of CheckList. We present the results in Table 4. Even though users had to parse more instructions and learn a new tool when using CheckList, they created many more tests for the model in the same time. Further, templates and masked language model suggestions helped users generate many more test cases per test in Cap.+templ. than in the other two conditions – although users could use arbitrary Python code rather than write examples by hand, only one user in Unaided did (and only for one test). 5i.e. have taken a graduate NLP course or equivalent. Unaided CheckList Cap. only Cap.+templ. #Tests 5.8˘ 1.1 10.2˘ 1.8 13.5˘ 3.4 #Cases/test 7.3˘ 5.6 5.0˘ 1.2 198.0˘ 96 #Capabilities tested 3.2˘ 0.7 7.5˘ 1.9 7.8˘ 1.1 Total severity 10.8˘ 3.8 21.7˘ 5.7 23.7˘ 4.2 #Bugs (sev ě 3q 2.2˘ 1.2 5.5˘ 1.7 6.2˘ 0.9 Table 4: User Study Results: first three rows indi- cate number of tests created, number of test cases per test and number of capabilities tested. Users report the severity of their findings (last two rows). Users explored many more capabilities on Cap. only and Cap.+templ. (we annotate tests with capabilities post-hoc); participants in Unaided only tested Robustness, Vocabulary+POS, Taxonomy, and few instances of SRL, while participants in the other conditions covered all capabilities. Users in Cap. only and Cap.+templ. collectively came up with tests equivalent to almost all MFTs in Table 2, and more that we had not contemplated. Users in Unaided and Cap. only often did not find more bugs because they lacked test case variety even when testing the right concepts (e.g. negation). At the end of the experiment, we ask users to evaluate the severity of the failures they observe on each particular test, on a 5 point scale6. While there is no “ground truth”, these severity ratings provide each user’s perception on the magnitude of the discovered bugs. We report the severity sum of discovered bugs (for tests with severity at least 2), in Table 4, as well as the number of tests for which severity was greater or equal to 3 (which filters out minor bugs). We note that users with Check- List (Cap. only and Cap.+templ.) discovered much more severe problems in the model (measured by total severity or # bugs) than users in the control condition (Unaided). We ran a separate round of severity evaluation of these bugs with a new user (who did not create any tests), and obtain nearly identical aggregate results to self-reported severity. The study results are encouraging: with a subset of CheckList, users without prior experience are able to find significant bugs in a SOTA model in only 2 hours. Further, when asked to rate different aspects of CheckList (on a scale of 1-5), users in- dicated the testing session helped them learn more about the model (4.7 ˘ 0.5), capabilities helped them test the model more thoroughly (4.5 ˘ 0.4), and so did templates (4.3 ˘ 1.1). 61 (not a bug), 2 (minor bug), 3 (bug worth investigating and fixing), 4 (severe bug, model may not be fit for production), and 5 (no model with this bug should be in production)."
            },
            {
                "sectionTitle": "5 Related Work",
                "text": "One approach to evaluate specific linguistic capa- bilities is to create challenge datasets. Belinkov and Glass (2019) note benefits of this approach, such as systematic control over data, as well as drawbacks, such as small scale and lack of resem- blance to “real” data. Further, they note that the majority of challenge sets are for Natural Language Inference. We do not aim for CheckList to replace challenge or benchmark datasets, but to comple- ment them. We believe CheckList maintains many of the benefits of challenge sets while mitigating their drawbacks: authoring examples from scratch with templates provides systematic control, while perturbation-based INV and DIR tests allow for testing behavior in unlabeled, naturally-occurring data. While many challenge sets focus on extreme or difficult cases (Naik et al., 2018), MFTs also focus on what should be easy cases given a capa- bility, uncovering severe bugs. Finally, the user study demonstrates that CheckList can be used ef- fectively for a variety of tasks with low effort: users created a complete test suite for sentiment analysis in a day, and MFTs for QQP in two hours, both revealing previously unknown, severe bugs. With the increase in popularity of end-to- end deep models, the community has turned to “probes”, where a probing model for linguistic phe- nomena of interest (e.g. NER) is trained on in- termediate representations of the encoder (Tenney et al., 2019; Kim et al., 2019). Along similar lines, previous work on word embeddings looked for cor- relations between properties of the embeddings and downstream task performance (Tsvetkov et al., 2016; Rogers et al., 2018). While interesting as analysis methods, these do not give users an under- standing of how a fine-tuned (or end-to-end) model can handle linguistic phenomena for the end-task. For example, while Tenney et al. (2019) found that very accurate NER models can be trained using BERT (96.7%), we show BERT finetuned on QQP or SST-2 displays severe NER issues. There are existing perturbation techniques meant to evaluate specific behavioral capabilities of NLP models such as logical consistency (Ribeiro et al., 2019) and robustness to noise (Belinkov and Bisk, 2018), name changes (Prabhakaran et al., 2019), or adversaries (Ribeiro et al., 2018). CheckList provides a framework for such techniques to sys- tematically evaluate these alongside a variety of other capabilities. However, CheckList cannot be directly used for non-behavioral issues such as data versioning problems (Amershi et al., 2019), label- ing errors, annotator biases (Geva et al., 2019), worst-case security issues (Wallace et al., 2019), or lack of interpretability (Ribeiro et al., 2016)."
            },
            {
                "sectionTitle": "6 Conclusion",
                "text": "While useful, accuracy on benchmarks is not suffi- cient for evaluating NLP models. Adopting princi- ples from behavioral testing in software engineer- ing, we propose CheckList, a model-agnostic and task-agnostic testing methodology that tests indi- vidual capabilities of the model using three differ- ent test types. To illustrate its utility, we highlight significant problems at multiple levels in the con- ceptual NLP pipeline for models that have “solved” existing benchmarks on three different tasks. Fur- ther, CheckList reveals critical bugs in commercial systems developed by large software companies, in- dicating that it complements current practices well. Tests created with CheckList can be applied to any model, making it easy to incorporate in current benchmarks or evaluation pipelines. Our user studies indicate that CheckList is easy to learn and use, and helpful both for expert users who have tested their models at length as well as for practitioners with little experience in a task. The tests presented in this paper are part of Check- List’s open source release, and can easily be in- corporated into existing benchmarks. More impor- tantly, the abstractions and tools in CheckList can be used to collectively create more exhaustive test suites for a variety of tasks. Since many tests can be applied across tasks as is (e.g. typos) or with minor variations (e.g. changing names), we ex- pect that collaborative test creation will result in evaluation of NLP models that is much more ro- bust and detailed, beyond just accuracy on held-out data. CheckList is open source, and available at https://github.com/marcotcr/checklist. Acknowledgments We would like to thank Sara Ribeiro, Scott Lund- berg, Matt Gardner, Julian Michael, and Ece Kamar for helpful discussions and feedback. Sameer was funded in part by the NSF award #IIS-1756023, and in part by the DARPA MCS program under Contract No. N660011924033 with the United States Office of Naval Research. https://github.com/marcotcr/checklist"
            },
            {
                "sectionTitle": "References",
                "text": "Saleema Amershi, Andrew Begel, Christian Bird, Rob DeLine, Harald Gall, Ece Kamar, Nachi Nagap- pan, Besmira Nushi, and Tom Zimmermann. 2019. Software engineering for machine learning: A case study. In International Conference on Software En- gineering (ICSE 2019) - Software Engineering in Practice track. IEEE Computer Society. Boris Beizer. 1995. Black-box Testing: Techniques for Functional Testing of Software and Systems. John Wiley & Sons, Inc., New York, NY, USA. Yonatan Belinkov and Yonatan Bisk. 2018. Synthetic and natural noise both break neural machine transla- tion. In International Conference on Learning Rep- resentations. Yonatan Belinkov and James Glass. 2019. Analysis methods in neural language processing: A survey. Transactions of the Association for Computational Linguistics, 7:49–72. Mor Geva, Yoav Goldberg, and Jonathan Berant. 2019. Are we modeling the task or the annotator? an inves- tigation of annotator bias in natural language under- standing datasets. In Empirical Methods in Natural Language Processing (EMNLP), pages 1161–1166. Mohit Iyyer, John Wieting, Kevin Gimpel, and Luke Zettlemoyer. 2018. Adversarial example generation with syntactically controlled paraphrase networks. In Proceedings of NAACL-HLT, pages 1875–1885. Najoung Kim, Roma Patel, Adam Poliak, Patrick Xia, Alex Wang, Tom McCoy, Ian Tenney, Alexis Ross, Tal Linzen, Benjamin Van Durme, et al. 2019. Prob- ing what different nlp tasks teach machines about function word comprehension. In Proceedings of the Eighth Joint Conference on Lexical and Compu- tational Semantics (* SEM 2019), pages 235–249. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692. Aakanksha Naik, Abhilasha Ravichander, Norman Sadeh, Carolyn Rose, and Graham Neubig. 2018. Stress Test Evaluation for Natural Language Infer- ence. In International Conference on Computa- tional Linguistics (COLING). Kayur Patel, James Fogarty, James A Landay, and Bev- erly Harrison. 2008. Investigating statistical ma- chine learning as a tool for software development. In Proceedings of the SIGCHI Conference on Hu- man Factors in Computing Systems, pages 667–676. ACM. Vinodkumar Prabhakaran, Ben Hutchinson, and Mar- garet Mitchell. 2019. Perturbation sensitivity analy- sis to detect unintended model biases. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter- national Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 5740–5745, Hong Kong, China. Association for Computational Lin- guistics. Pranav Rajpurkar, Robin Jia, and Percy Liang. 2018. Know what you don’t know: Unanswerable ques- tions for SQuAD. In Proceedings of the 56th An- nual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 784– 789, Melbourne, Australia. Association for Compu- tational Linguistics. Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. 2016. Squad: 100,000+ questions for machine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods in Natu- ral Language Processing, pages 2383–2392. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. 2019. Do imagenet classifiers generalize to imagenet? In International Confer- ence on Machine Learning, pages 5389–5400. Marco Tulio Ribeiro, Carlos Guestrin, and Sameer Singh. 2019. Are red roses red? evaluating con- sistency of question-answering models. In Proceed- ings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6174–6184. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. Why should i trust you?: Explain- ing the predictions of any classifier. In Proceed- ings of the 22nd ACM SIGKDD international con- ference on knowledge discovery and data mining, pages 1135–1144. ACM. Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. 2018. Semantically equivalent adversarial rules for debugging nlp models. In Association for Computational Linguistics (ACL). Anna Rogers, Shashwath Hosur Ananthakrishna, and Anna Rumshisky. 2018. What’s in your embedding, and how it predicts task performance. In Proceed- ings of the 27th International Conference on Com- putational Linguistics, pages 2690–2703, Santa Fe, New Mexico, USA. Association for Computational Linguistics. Barbara Rychalska, Dominika Basaj, Alicja Gosiewska, and Przemysław Biecek. 2019. Models in the wild: On corruption robustness of neural nlp systems. In International Conference on Neural Information Processing, pages 235–247. Springer. Sergio Segura, Gordon Fraser, Ana B Sanchez, and An- tonio Ruiz-Cortés. 2016. A survey on metamorphic testing. IEEE Transactions on software engineering, 42(9):805–824. Ian Tenney, Dipanjan Das, and Ellie Pavlick. 2019. BERT rediscovers the classical NLP pipeline. In https://www.microsoft.com/en-us/research/publication/software-engineering-for-machine-learning-a-case-study/ https://www.microsoft.com/en-us/research/publication/software-engineering-for-machine-learning-a-case-study/ https://doi.org/10.18653/v1/D19-1578 https://doi.org/10.18653/v1/D19-1578 https://doi.org/10.18653/v1/P18-2124 https://doi.org/10.18653/v1/P18-2124 https://www.aclweb.org/anthology/C18-1228 https://www.aclweb.org/anthology/C18-1228 https://doi.org/10.18653/v1/P19-1452 Proceedings of the 57th Annual Meeting of the Asso- ciation for Computational Linguistics, pages 4593– 4601, Florence, Italy. Association for Computational Linguistics. Yulia Tsvetkov, Manaal Faruqui, and Chris Dyer. 2016. Correlation-based intrinsic evaluation of word vec- tor representations. In Proceedings of the 1st Work- shop on Evaluating Vector-Space Representations for NLP, pages 111–115, Berlin, Germany. Associ- ation for Computational Linguistics. Eric Wallace, Shi Feng, Nikhil Kandpal, Matt Gardner, and Sameer Singh. 2019. Universal adversarial trig- gers for attacking and analyzing nlp. In Proceed- ings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th Inter- national Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP), pages 2153–2162. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2019a. Superglue: A stickier benchmark for general-purpose language un- derstanding systems. In Advances in Neural Infor- mation Processing Systems, pages 3261–3275. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. 2019b. GLUE: A multi-task benchmark and analysis plat- form for natural language understanding. In Inter- national Conference on Learning Representations. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pier- ric Cistac, Tim Rault, R’emi Louf, Morgan Funtow- icz, and Jamie Brew. 2019. Huggingface’s trans- formers: State-of-the-art natural language process- ing. ArXiv, abs/1910.03771. Tongshuang Wu, Marco Tulio Ribeiro, Jeffrey Heer, and Daniel S Weld. 2019. Errudite: Scalable, repro- ducible, and testable error analysis. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 747–763. https://doi.org/10.18653/v1/W16-2520 https://doi.org/10.18653/v1/W16-2520 https://openreview.net/forum?id=rJ4km2R5t7 https://openreview.net/forum?id=rJ4km2R5t7"
            }
        ]
    }
}