Talking-head Generation with Rhythmic Head
Motion
Lele Chen† ID , Guofeng Cui† ID , Celong Liu‡ ID , Zhong Li‡ ID , Ziyi Kou† ID , Yi
Xu‡ ID , and Chenliang Xu† ID
† University of Rochester ‡ OPPO US Research Center
lchen63@cs.rochester.edu
Abstract. When people deliver a speech, they naturally move heads,
and this rhythmic head motion conveys prosodic information. However,
generating a lip-synced video while moving head naturally is challenging.
While remarkably successful, existing works either generate still talking-
face videos or rely on landmark/video frames as sparse/dense mapping
guidance to generate head movements, which leads to unrealistic or un-
controllable video synthesis. To overcome the limitations, we propose a
3D-aware generative network along with a hybrid embedding module and
a non-linear composition module. Through modeling the head motion
and facial expressions1 explicitly, manipulating 3D animation carefully,
and embedding reference images dynamically, our approach achieves con-
trollable, photo-realistic, and temporally coherent talking-head videos
with natural head movements. Thoughtful experiments on several stan-
dard benchmarks demonstrate that our method achieves significantly
better results than the state-of-the-art methods in both quantitative and
qualitative comparisons. The code is available on https://github.com/
lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion.
1 Introduction
People naturally emit head movements when they speak, which contain non-
verbal information that helps the audience comprehend the speech content [4,15].
Modeling the head motion and then generating a controllable talking-head video
are valuable problems in computer vision. For example, it can benefit the re-
search of adversarial attacks in security or provide more training samples for
supervised learning approaches. Meanwhile, it is also crucial to real-world appli-
cations, such as enhancing speech comprehension for hearing-impaired people,
and generating virtual characters with synchronized facial movements to speech
audio in movies/games.
We humans can infer the visual prosody from a short conversation with
speakers [22]. Inspired by that, in this paper, we consider such a task: given a
1 In our setting, facial expression means facial movement(e.g., blinks, and lip & chin
movements).
ar
X
iv
:2
00
7.
08
54
7v
1 
 [
cs
.C
V
] 
 1
6 
Ju
l 
20
20
https://orcid.org/0000-0002-7073-0450
https://orcid.org/0000-0002-7994-915X
https://orcid.org/0000-0002-6108-7010
https://orcid.org/0000-0002-7416-1216
https://orcid.org/0000-0002-9916-0930
https://orcid.org/0000-0003-2126-6054
https://orcid.org/0000-0002-2183-822X
https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion
https://github.com/lelechen63/Talking-head-Generation-with-Rhythmic-Head-Motion
2 L. Chen, et al.
Composed FramesSynthesized region
Original Frames
Audio or Text
“ECCV” tim
e
Only generate a small region;
Need original video frames as input;
Can not have new head movements.
Audio
K reference frames
3 secs head motion
Generate whole image frame (e.g., background);
Only need input K reference images;
New & controllable head movements;
Controllable facial expression.
Synthesized Frames
Others Ours
Fig. 1. The comparisons of other methods [27,13] and our method. The green arrows
denote inputs. [27,13] require whole original video frames as input and can only edit
a small region (e.g., lip region) on the original video frames. In contrast, through
learning the appearance embedding from K reference frames and predicting future head
movements using the head motion vector extracted from the 3-sec reference video, our
method generates whole future video frames with controllable head movements and
facial expressions.
short video 2 of the target subject and an arbitrary reference audio, generating a
photo-realistic and lip-synced talking-head video with natural head movements.
Similar problems [8,39,26,30,7,35,37,31] have been explored recently. However,
several challenges on how to explicitly use the given video and then model the
apparent head movements remain unsolved. In the rest of this section, we discuss
our technical contributions concerning each challenge.
The deformation of a talking-head consists of his/her intrinsic subject traits,
head movements, and facial expressions, which are highly convoluted. This com-
plexity stems not only from modeling face regions but also from modeling head
motion and background. While previous audio-driven talking-face generation
methods [9,35,39,6,26,7] could generate lip-synced videos, they omit the head
motion modeling, thus can only generate still talking-face with expressions un-
der a fixed facial alignment. Other landmark/image-driven generation meth-
ods [37,31,32,33,35] can synthesize moving-head videos relying on the input facial
landmarks or video frames as guidance to infer the convoluted head motion and
facial expressions. However, those methods fail to output a controllable talking-
head video (e.g., control facial expressions with speech), which greatly limits
their use. To address the convoluted deformation problem, we propose a simple
but effective method to decompose the head motion and facial expressions.
Another challenge is exploiting the information contained in the reference im-
age/video. While the few-shot generation methods [37,31,20,36] can synthesize
videos of unseen subjects by leveraging K reference images, they only utilize
global appearance information. There remains valuable information underex-
plored. For example, we can infer the individual’s head movement characteris-
tics by leveraging the given short video. Therefore, we propose a novel method
to extrapolate rhythmic head motion based on the short input video and the
2 E.g., a 3-sec video. We use it to learn head motion and only sample K images as
reference frames.
Talking-head Generation with Rhythmic Head Motion 3
conditioned audio, which enables generating a talking-head video with natural
head poses. Besides, we propose a novel hybrid embedding network dynamically
aggregating information from the reference images by approximating the relation
between the target image with the reference images.
People are sensitive to any subtle artifacts and perceptual identity changes
in a synthesized video, which are hard to avoid in GAN-based methods. 3D
graphics modeling has been introduced by [13,17,38] in GAN-based methods
due to its stability. In this paper, we employ the 3D modeling along with a novel
non-linear composition module to alleviate the visual discontinuities caused by
apparent head motion or facial expressions.
Combining the above features, which are designed to overcome limitations of
existing methods, our framework can generate a talking-head video with natural
head poses that conveys the given audio signal. We conduct extensive experi-
mental validation with comparisons to various state-of-the-art methods on sev-
eral benchmark datasets (e.g., VoxCeleb2 [9] and LRS3-TED [1] datasets) under
several different settings (e.g., audio-driven and landmark-driven). Experimental
results show that the proposed framework effectively addresses the limitations
of those existing methods.
2 Related Work
2.1 Talking-head Image Generation
The success of graphics-based approaches has been mainly limited to synthe-
sizing talking-head videos for a specific person [14,2,5,19,27,13]. For instance,
Suwajanakorn et al. [27] generate a small region (e.g., lip region, see Fig. 1)3
and compose it with a retrieved frame from a large video corpus of the tar-
get person to produce photo-realistic videos. Although it can synthesize fairly
accurate lip-synced videos, it requires a large amount of video footage of the
target person to compose the final video. Moreover, this method can not be gen-
eralized to an unseen person due to the rigid matching scheme. More recently,
video-to-video translation has been shown to be able to generate arbitrary faces
from arbitrary input data. While the synthesized video conveys the input speech
signal, the talking-face generation methods [24,8,39,26,30,7] can only generate
facial expressions without any head movements under a fixed alignment since
the head motion modeling has been omitted. Talking-head methods [37,31,35]
can generate high-quality videos guided by landmarks/images. However, these
methods can not generate controllable video since the facial expressions and
head motion are convoluted in the guidance, e.g., using audio to drive the gen-
eration. In contrast, we explicitly model the head motion and facial expressions
in a disentangled manner, and propose a method to extrapolate rhythmic head
motion to enable generating future frames with natural head movements.
3 We intent to highlight the different between [27,13] and our work. While they
generate high-quality videos, they can not disentangle the head motion and facial
movement due to their intrinsic limitations.
4 L. Chen, et al.
2.2 Related Techniques
Embedding Network refers to the external memory module to learn com-
mon feature over few reference images of the target subjects. And this net-
work is critical to identity-preserving performance in generation task. Previous
works [8,7,30,26] directly use CNN encoder to extract the feature from refer-
ence images, then concatenate with other driven vectors and decode it to new
images. However, this encoder-decoder structure suffers identity-preserving prob-
lem caused by the deep convolutional layers. Wiles et al. [35] propose an embed-
ding network to learn a bilinear sampler to map the reference frames to a face
representation. More recently, [37,36,31] compute part of the network weights dy-
namically based on the reference images, which can adapt to novel cases quickly.
Inspired by [31], we propose a hybrid embedding network to aggregate appear-
ance information from reference images with apparent head movements.
Image matting function has been well explored in image/video generation
task [29,32,24,7,31]. For instance, Pumarola et al. [24] compute the final output
image by Î = (1−A)∗C+A∗ Ir, where Ir, A and C are input reference image,
attention map and color mask, respectively. The attention map A indicates to
what extend each pixel of Ir contributes to the output image Î. However, this
attention mechanism may not perform well if there exits a large deformation
between reference frame Ir and target frame Î. Wang et al. [33] use estimated
optic flow to warp the Ir to align with Î, which is computationally expensive and
can not estimate the rotations in talking-head videos. In this paper, we propose
a 3D-aware solution along with a non-linear composition module to better tackle
the misalignment problem caused by apparent head movements.
3 Method
3.1 Problem Formulation
We introduce a neural approach for talking-head video generation, which takes
as input sampled video frames, y1:τ ≡ y1, ...,yτ , of the target subject and a
sequence of driving audio, xτ+1:T ≡ xτ+1, ...,xT , and synthesizes target video
frames, ŷτ+1:T ≡ ŷτ+1, ..., ŷT , that convey the given audio signal with realistic
head movements. To explicitly model facial expressions and head movements, we
decouple the full model into three sub-models: a facial expression learner (Ψ), a
head motion learner (Φ), and a 3D-aware generative network (Θ). Specifically,
given an audio sequence xτ+1:T and an example image frame ytM , Ψ generates
the facial expressions p̂τ+1:T . Meanwhile, given a short reference video y1:τ and
the driving audio xτ+1:T , Φ extrapolates natural head motion ĥτ+1:T to ma-
nipulate the head movements. Then, the Θ generates video frames ŷτ+1:T using
p̂τ+1:T , ĥτ+1:T , and y1:τ . Thus, the full model is given by:
ŷτ+1:T = Θ(y1:τ , p̂τ+1:T , ĥτ+1:T ) = Θ(y1:τ ,Ψ(ytM ,xτ+1:T ),Φ(y1:τ ,xτ+1:T )) .
The proposed framework (see Fig. 2) aims to exploit the facial texture and
head motion information in y1:τ and maps the driving audio signal to a sequence
of generated video frames ŷτ+1:T .
Talking-head Generation with Rhythmic Head Motion 5
…… … …
Facial Expression 
Learner (    )
Head Motion 
Learner (    )
Unprojection
Projector
Hybrid
Embedding
Motion
Matcher Non-Linear
Composition
Fig. 2. The overview of the framework. Ψ and Φ are introduced in Sec. 3.2 and Sec. 3.3,
respectively. The 3D-aware generative network consists of a 3D-Aware module (green
part, see Sec. 4.1), a hybrid embedding module (blue part, see Sec. 4.2), and a non-
linear composition module (orange part, see Sec 4.3).
3.2 The Facial Expression Learner
The facial expression learner Ψ (see Fig. 2 red block) receives as input the
raw audio xτ+1:T , and a subject-specific image ytM (see Sec. 4.1 about how
to select ytM), from which we extract the landmark identity feature ptM . The
desired outputs are PCA components p̂τ+1:T that convey the facial expression.
During training, we mix xτ+1:T with a randomly selected noise file in 6 to 30
dB SNRs with 3 dB increments to increase the network robustness. At time
step t, we encode audio clip xt−3:t+4 (0.04× 7 Sec) into an audio feature vector,
concatenate it with the encoded reference landmark feature, and then decode the
fused feature into PCA components of target expression p̂t. During inference,
we add the identity feature ptM back to the resultant facial expressions p̂τ+1:T
to keep the identity information.
(a) (b)
Frame number Frame number Frame number
[R
,T
]c
oe
ffi
ci
en
t 
#0
[R
,T
]c
oe
ffi
ci
en
t 
#1
[R
,T
]c
oe
ffi
ci
en
t 
#5
Fig. 3. (a) shows the motion disentangling process. To avoid the noise caused by
non-rigid deformation, we select 27 points (red points), which contain less non-rigid
deformation. (b) shows the head movement coefficients of two randomly-selected iden-
tities (each with four different videos). PID, VID indicate identity id and video id,
respectively.
6 L. Chen, et al.
3.3 The Head Motion Learner
To generate talking-head videos with natural head movements, we introduce a
head motion learner Φ, which disentangles the reference head motion h1:τ from
the input reference video clip y1:τ and then predicts the head movements ĥτ+1:T
based on the driving audio x1:T and the disentangled motion h1:τ .
Motion Disentangler. Rather than disentangling the head motion problem
in image space, we learn the head movements h in 3D geometry space (see
Fig. 3(a)). Specifically, we compute the transformation matrix [R,T] ∈ R6 to
represent h, where we omit the camera motion and other factors. At each time
step t, we compute the rigid transformation [R,T]t between lt and canonical 3D
facial landmark l∗, which is l∗ ≈ pt = Rtlt+Tt, where ≈ denotes aligned. After
transformation, the head movement information is removed, and the resultant
pt only carries the facial expressions information.
The natural head motion extrapolation. We randomly select two sub-
jects (each with four videos) from VoxCeleb2 [9] and plot the motion coefficients
in Fig. 3(b). We can find that one subject (same PID, different VID) has sim-
ilar head motion patterns in different videos, and different subjects (different
PID) have much different head motion patterns. To explicitly learn the indi-
vidual’s motion from h1:τ and the audio-correlated head motion from x1:T , we
propose a few-shot temporal convolution network Φ, which can be formulated
as: ĥτ+1:T = Φ(f(h1:τ ,x1:τ ),xτ+1:T ). In details, the encoder f first encodes
h1:τ and x1:τ to a high-dimensional reference feature vector, and then trans-
forms the reference feature to network weights w using a multi-layer perception.
Meanwhile, xτ+1:T is encoded by several temporal convolutional blocks. Then,
the encoded audio feature is processed by a multi-layer perception, where the
weights are dynamically learnable w, to generate ĥτ+1:T .
3.4 The 3D-Aware Generative Network
To generate controllable videos, we propose a 3D-aware generative network Θ,
which fuses the head motion hτ+1:T , and facial expressions pτ+1:T with the ap-
pearance information in y1:τ to synthesise target video frames, ŷτ+1:T . The Θ
consists of three modules: a 3D-Aware module, which manipulates the head mo-
tion to reconstruct an intermediate image that carries desired head pose using
a differentiable unprojection-projection step to bridge the gap between images
with different head poses. Comparing to landmark-driven methods [37,31,33], it
can alleviate the overfitting problem and make the training converge faster; a
Hybrid Embedding module, which is proposed to explicitly embed texture fea-
tures from different appearance patterns carried by different reference images;
a Non-Linear Composition module, which is proposed to stabilize the back-
ground and better preserve the individual’s appearance. Comparing to image
matting function [24,39,37,33,7], our non-linear composition module can synthe-
size videos with natural facial expression, smoother transition, and a more stable
background.
Talking-head Generation with Rhythmic Head Motion 7
4 3D-Aware Generation
4.1 3D-Aware Module
3D Unprojection We assume that the image with frontal face contains more
appearance information. Given a short reference video clip y1:τ , we compute
the head rotation R1:τ and choose the most visible frame ytM with minimum
rotation, such that RtM → 0. Then we feed it to an unprojection network to
obtain a 3D meshM. The Unprojection network is a U-net structure network [12]
and we pretrain it on 300W-LP [40] dataset. After pretrianing, we fix the weights
and use it to transfer the input image ytM into the textured 3D mesh M. The
topology of M will be fixed for all frames in one video, we denote it as FM.
3D Projector In order to get the projected image ỹt from the 3D face mesh
M, we need to compute the correct pose for M at time t. Suppose M is recon-
structed from frame ytM , (1 ≤ tM ≤ τ), the position of vertices of M at time t
can be computed by:
VMt = R
−1
t (RtMV
M
tM
+ TtM −Tt) , (1)
where VMtM denotes the position of vertices ofM at time tM. Hence,M at time t
can be represented as
[
VMt ,FM
]
. Then, a differentiable rasterizer [21] is applied
here to render ỹt from
[
VMt ,FM
]
. After rendering, ỹt carries the same head
pose as the target frame yt and the same expression as ytM .
Motion Matcher We randomly select K frames y1:K from y1:τ as reference
images. To infer the prior knowledge about the background of the target image
from y1:K , we propose a motion matcher, M(ht,h1:K ,y1:K), which matches
a frame yts from y1:K with the nearest background as yt by comparing the
similarities between ht with h1:K . To solve this background retrieving problem,
rather than directly computing the similarities between facial landmarks, we
compute the geometry similarity cost ct using the head movement coefficients
and choose the yts with the least cost ct. That is:
ct = min
k=1,2,...K
‖ht − hk‖22 . (2)
The matched yts is passed to the non-linear composition module (see Sec. 4.3)
since it carries the similar background pattern as yt. During training, we manip-
ulate the selection of yts by increasing the cost ct, which makes the non-linear
composition module more robust to different yts .
4.2 Hybrid Embedding Module
Instead of averaging the K reference image features [37,8,35], we design a hybrid
embedding mechanism to dynamically aggregate the K visual features.
Recall that we obtain the facial expression pt (Sec. 3.2) and the head motion
ht (Sec. 3.3) for time t. Here, we combine them to landmark lt and transfer it to
a gray-scale image. Our hybrid embedding network (Fig. 4) consists of four sub-
networks: an Activation Encoder (EA, green part) to generate the activation
8 L. Chen, et al.
…
…
…
Information Exchange
…
Attention
Combine
ConvGate
ConvGate
Attention
Combine … …
Information Exchange
…
Reshape
ConvGate
Fig. 4. The network structure of hybrid embedding module. Green, yellow, blue, pink
parts are activation encoder EA, landmark encoder EL, image encoder EI , and fusion
network EF , respectively. Inside information exchange block, we first concatenate fea-
tures from two modality, and then apply two bottleneck layer to decrease the channel
dimension. Inside ConvGate block, we first reshape the feature shape and then ap-
ply two bottleneck layer to decrease the channel dimension. The attention combine is
shown in Eq. 6.
map between the query landmark lt and reference landmarks l1:K ; an Image
Encoder (EI , blue part) to extract the image feature ey; a Landmark Encoder
(EL, yellow part) to extract landmark feature el; a fusion network (EF , pink
part) to aggregate image feature and landmark feature into parameter set θ.
The overview of the embedding network is performed by:
αk = softmax(EA(lk)� EA(lt)) , k ∈ [1,K] , (3)
ey = EI(y1:K ,α1:K) , el = EL(l1:K ,α1:K) , (4)
θ = EF (el, ey) , (5)
where � denotes element-wise multiplication. We regard the activation map αk
as the activation energy between lk and lt, which approximates the similarity
between yk and yt.
We observe that different referent images may carry different appearance
patterns, and those appearance patterns share some common features with the
reference landmarks (e.g., head pose, and edges). Assuming that knowing the
information of one modality can better encode the feature of another modal-
ity [28], we hybrid the information between EI and EL. Specifically, we use two
convolutional layers to extract the feature map qy1:K and ql1:K , and then forward
them to an information exchange block to refine the features. After exchanging
the information, the hybrid feature is concatenated with the original feature
qy1:K . Then, we pass the concatenated feature to a bottleneck convolution layer
to produce the refined feature q′y1:K . Meanwhile, we apply a ConvGate block
on qy1:K to self-aggregate the features from K references, and then combine the
gated feature with the refined feature using learnable activation map αk. This
Talking-head Generation with Rhythmic Head Motion 9
attention combine step can be formulated as:
qy =
K∑
i=1
(α1:K � q′y1:K ) + ConvGate(qy1:K ) . (6)
Then, by applying several convolutional layers to the aggregated feature qy, we
obtain the image feature vector ey. Similarly, the landmark feature vector el can
also be produced.
The Fusion network EF consists of three blocks: a N -layer image feature
encoding block EFI , a landmark feature encoding block EFL , and a multi-layer
perception block EFP . Thus, we can rewrite Eq. 5 as:
{θiγ , θ
i
β , θ
i
S}i∈[1,N ] = EFP (softmax(EFL(el))� EFI (ey)) , (7)
where {θiγ , θiβ , θ
i
S}i∈[1,N ] are the learnable network weights in the non-linear com-
position module in Sec. 4.3.
Fig. 5. The artifact examples produced by the image matting function. The blue dashed
box shows the color map C, reference image Ir, and attention map A , respectively.
The blue box shows the image matting function (see Sec. 2.2) followed by the final
result with its zoom-in artifact area. Two other artifact examples are on the right side.
4.3 Non-Linear Composition Module
Since the projected image ỹt carries valuable facial texture with better alignment
to the ground-truth, and the matched image yts contains similar background
pattern, we input them to our composition module to ease the burden of the
generation. We use FlowNet [32] to estimate the flow between l̃t and l̂t, and
warp the projected image to obtain ỹ′t. Meanwhile, the FlowNet also outputs
an attention map αỹt . Similarly, we obtain the warped matched image and its
attention map [y′ts ,αyts ]. Although image matting function is a popular way
to combine the warped image and the raw output of the generator [31,32,7,24]
using these attention maps, it may generate apparent artifacts (Fig. 5) caused
by misalignment, especially in the videos with apparent head motion. To solve
this problem, we introduce a nonlinear combination module (Fig. 6).
The decoder G consists of N layers and each layer contains three parallel
SPADE blocks [23]. Inspired by [31], in the ith layer of G, we first convolve
[θiγ , θ
i
β ] with φ
i
lt
to generate the scale and bias map {γi, βi}φi
lt
of SPADE (green
box) to denormalize the appearance vector eiy. This step aims to sample the face
10 L. Chen, et al.
Concat.
Convs
Concat.
Convs
Flow
W
Flow
W
Convs SPADE
SPADE
SPADE sum
W : Flow Warping
: Output of the layer of Convs
Convs: Convolutional layers
Fig. 6. The network structure of the non-linear composition module. The network
weights {θiS , θ
i
γ , θ
i
β}i∈[1,N ] and ey are learned in embedding network (Sec. 4.2). To
better encode l̂t with extra knowledge learned from the embedding network, we lever-
age {θiS}i∈[1,N ] as the weights of the convolutional layers (green arrow) and save all
the intermediate features {φilt}i∈[1,N ]. Similarly, we obtain {φ
i
ỹt
, φiyts }i∈[1,N ] with fixed
convolutional weights (red arrow). Then, we input {φiỹt , φ
i
yts
, φilt}i∈[1,N ] into the cor-
responding block of the decoder G to guide the image generation.
representation eiy towards the target pose and expression. In other two SPADE
blocks (orange box), the scale and bias map {γi, βi}φyts ,φỹt are generated by
fixed weights convoluted on φyts and φỹt , respectively. Then, we sum up the
three denomalized features and upsample them with a transposed convolution.
We repeat this non-linear combination module in all layers of G and apply a
hyperbolic tangent activation layer at the end of G to output fake image ŷt. Our
experiment results (ablation study in Sec. 6) show that this parallel non-linear
combination is more robust to the spatial-misalignment among the input images,
especially when there is an apparent head motion.
4.4 Objective Function
The Multi-Scale Discriminators [33] are employed to differentiate the real and
synthesized video frames. The discriminators D1 and D2 are trained on two
different input scales, respectively. Thus, the best generator G∗ is found by
solving the following optimization problem:
G∗ = min
G
(
max
D1,D2
∑
k=1,2
LGAN(G,Dk)
+ λFM
∑
k=1,2
LFM(G,Dk)
)
+ λPCTLPCT(G) + λWLW (G) ,
(8)
where LFM and LW are the feature matching loss and flow loss proposed
in [32]. The LPCT is the VGG19 [25] perceptual loss term, which measures the
perceptual similarity. The λFM, λPCT, and λW control the importance of the
four loss terms.
5 Experiments Setup
Dataset. We quantitatively and qualitatively evaluate our approach on three
datasets: LRW [10], VoxCeleb2 [9], and LRS3-TED [1]. The LRW dataset con-
Talking-head Generation with Rhythmic Head Motion 11
Table 1. Quantitative results of different audio to video methods on LRW dataset and
VoxCeleb2 dataset. Our model mentioned in this table are trained from scratch. We
bold each leading score.
Method Audio-driven
Dataset LRW VoxCeleb2
LMD↓ CSIM↑ SSIM↑ FID↓ LMD↓ CSIM↑ SSIM↑ FID↓
Chung et al. [8] 3.15 0.44 0.91 132 5.4 0.29 0.79 159
Chen et al. [7] 3.27 0.38 0.84 151 4.9 0.31 0.82 142
Vougioukas et al. [30] 3.62 0.35 0.88 116 6.3 0.33 0.85 127
Ours (K=1) 3.13 0.49 0.76 62 3.37 0.42 0.74 47
Table 2. Quantitative results of different landmark to video methods on LRS3-TED
and VoxCeleb2 datasets. Our model mentioned in this table are trained from scratch.
We bold each leading score. The number after each method denotes the K frames of
the target subject.
Method Landmark-driven
Dataset LRS3-TED VoxCeleb2
SSIM↑ CSIM↑ FID↓ SSIM↑ CSIM↑ FID↓
Wiles et al. [35] 0.57 0.28 172 0.65 0.31 117.3
Chen et al. [7] 0.66 0.31 294 0.61 0.39 107.2
Zakharov et al. [37] (K=1) 0.56 0.27 361 0.64 0.42 88.0
Wang et al. [31] (K=1) 0.59 0.33 161 0.69 0.48 59.4
Ours (K=1) 0.71 0.37 354 0.71 0.44 40.8
Zakharov et al. [37] (K=8) 0.65 0.32 284 0.71 0.54 62.6
Wang et al. [31] (K=8) 0.68 0.36 144 0.72 0.53 42.6
Ours(K=8) 0.76 0.41 324 0.71 0.50 37.1
Zakharov et al. [37](K=32) 0.72 0.41 154 0.75 0.54 51.8
Wang et al. [31](K=32) 0.69 0.40 132 0.73 0.60 41.4
Ours (K=32) 0.79 0.44 122 0.78 0.58 35.7
sists of 500 different words spoken by hundreds of different speakers in the wild
and the VoxCeleb2 contains over 1 million utterances for 6,112 celebrities. The
videos in LRS3-TED [1] contain more diverse and challenging head movements
than the others. We follow same data split as [9,10,1].
Implementation Details4. We follow the same training procedure as [32].
We adopt ADAM [18] optimizer with lr = 2 × 10−4 and (β1, β2) = (0.5, 0.999).
During training, we select K = 1, 8, 32 in our embedding network. The τ in all
experiments is 64. The λFM = λPCT = λPCT = 10 in Eq. 8.
4 All experiments are conducted on an NVIDIA DGX-1 machine with 8 32GB V100
GPUs. It takes 5 days to converge the training on VoxCeleb2/LRS3-TED since they
contain millions of video clips. It takes less than 1 day to converge the training on
the GRID/CREMA. For more details, please refer to supplemental materials.
12 L. Chen, et al.
Ours Vougioukas et al.
Fig. 7. Comparison of our model with Vougioukas et al. [30] on CREMA-D testing set.
The orange dashed box shows reference images. Our model is pretrained on VoxCeleb2
training set and then finetuned on CREMA-D.
6 Results and Analysis
Evaluation Metrics. We use several criteria for quantitative evaluation. We
use Frchet Inception Distance (FID) [16], mostly quantifying the fidelity of syn-
thesized images, structured similarity (SSIM) [34], commonly used to measure
the low-level similarity between the real images and generated images. To eval-
uate the identity preserving ability, same as [37], we use CSIM, which com-
putes the cosine similarity between embedding vectors of the state-of-the-art face
recognition network [11] for measuring identity mismatch. To evaluate whether
the synthesized video contains accurate lip movements that correspond to the
input condition, we adopt the evaluation matrix Landmarks Distance (LMD)
proposed in [6]. Additionally, we conduct user study to investigate the visual
quality of the generated videos including lip-sync performance.
Fig. 8. We generate video frames on two real world images. The facial expressions
are controlled by audio signal and the head motion is controlled by our head motion
learner. The orange dashed box indicate reference frames.
Comparison with Audio-driven Methods. We first consider such a sce-
nario that takes audio and one frame as inputs and synthesizes a talking head
saying the speech, which has been explored in Chung et al. [8], Wiles et al. [35],
Chen et al. [7], Song et al. [26], Vougioukas et al. [30], and Zhou et al. [39]. Com-
paring with their methods, our method is able to generate vivid videos including
Talking-head Generation with Rhythmic Head Motion 13
controllable head movements and natural facial expressions. We show the videos
driven by audio in the supplemental materials. Tab. 1 shows the quantitative
results. For a fair comparison, we only input one reference frame. Note that
other methods require pre-processing including affine transformation and crop-
ping under a fixed alignment. And we do not add such constrains, which leads to
a lower SSIM matrix value (e.g., complex backgrounds). We also compare the fa-
cial expression generation (see Fig. 7)5 from audio with Vougioukas et al. [30] on
CREMA-D dataset [3]. Furthermore, we show two generation examples in Fig. 8
driven by audio input, which demonstrates the generalizability of our network.
Ground Truth Zakharov et al.Chen et al.Whiles et al. Wang et al. Ours Ground Truth Zakharov et al.Chen et al. Wang et al. Ours
Reference
frame
Reference
frame Whiles et al.
VoxCeleb2
LRS3
VoxCeleb2
LRS3
Fig. 9. Comparison on LRS3-TED and VoxCeleb2 testing set. We can find that our
method can generate lip-synced video frames while preserving the identity information.
Comparison with Visual-driven Methods. We also compare our ap-
proach with state-of-the-art landmark/image-driven generation methods: Za-
kharov et al.[37], Wiles et al. [35], and Wang et al. [31] on VoxCeleb2 and
LRS3-TED datasets. Fig. 9 shows the visual comparison, from where we can
find that our methods can synthesize accurate lip movement while moving the
head accurately. Comparing with other methods, ours performs much better es-
pecially when there is a apparent deformation between the target image and
reference image. We attribute it to the 3D-aware module, which can guide the
network with rough geometry information. We also show the quantitative re-
sults in Tab. 2, which shows that our approach achieves best performance in
most of the evaluation matrices. It is worth to mention that our method out-
performs other methods in terms of the CSIM score, which demonstrates the
generalizability of our method. We choose different K here to better understand
our matching scheme. We can see that with larger K value, the inception per-
formance (FID and SSIM) can be improved. Note that we only select K images
from the first 64 frames, and Zakharov et al. [37] and Wang et al. [31] select K
images from the whole video sequence, which arguably gives other methods an
advantage.
5 We show the raw output from the network in Fig. 7. Due to intrinsic limitations,
[30] only generates facial regions with a fixed scale.
14 L. Chen, et al.
full modelw/o warpingw/o 3D-Aware w/o hybrid embeddingground truthreference image w/o non-linear comp.
Fig. 10. Visualization of ablation studies. We show the artifacts using red circles/boxes.
Ablation Studies. We conduct thoughtful ablation experiments to study the
contributions of each module we introduced in Sec. 3 and Sec. 4. The ablation
studies are conducted on VoxCeleb2 dataset. As shown in Fig. 10, each com-
ponent contributes to the full model. For instance, from the third column, we
can find that the identity preserving ability drops dramatically if we remove
the 3d-aware module. We attribute this to the valuable highly-aligned facial
texture information provided 3D-aware module, which could stabilize the GAN
training and lead to a faster convergence. Another interesting case is that if we
remove our non-linear composition block or warping operation, the synthesized
images may contain artifacts in the area near face edges or eyes. We attribute
to the alignment issue caused by head movements. Please refer to supplemental
materials for more quantitative results on ablation studies.
Fig. 11. The statistics of
user studies. The scores
are normalized to 1.
User Studies. To compare the photo-realism and
faithfulness of the translation outputs, we perform hu-
man evaluation on videos generated by both audio
and landmarks on videos randomly sampled from dif-
ferent testing sets, including LRW [10], VoxCeleb2 [9],
and LRS3-TED [1]. Our results including: synthesized
videos conditioned on audio input, videos conditioned
on landmark guidance, and the generated videos that
facial expressions are controlled by audio signal and
the head movements are controlled by our motion
learner. Human subjects evaluation is conducted to
investigate the visual qualities of our generated results
compared with Wiles et al. [35], Wang et al. [31], Za-
kharov et al. [37] in terms of two criteria: whether
participants could regard the generated talking faces
as realistic and whether the generated talking head
temporally sync with the corresponding audio. The details of User Studies are
described in the supplemental materials. From Fig. 11, we can find that all our
methods outperform other two methods in terms of the extent of synchroniza-
tion and authenticity. It is worth to mention that the videos synthesized with
head movements (ours, K=1) learned by the motion learner achieve much better
Talking-head Generation with Rhythmic Head Motion 15
performance that the one without head motion, which indicate that the partic-
ipants prefer videos with natural head movements more than videos with still
face.
7 Conclusion and Discussion
In this paper, we propose a novel approach to model the head motion and fa-
cial expressions explicitly, which can synthesize talking-head videos with natural
head movements. By leveraging a 3d aware module, a hybrid embedding mod-
ule, and a non-linear composition module, the proposed method can synthesize
photo-realistic talking-head videos.
Although our approach outperforms previous methods, our model still fails in
some situations. For example, our method struggles synthesizing extreme poses,
especially there is no visual clues in the given reference frames. Furthermore,
we omit modeling the camera motions, light conditions, and audio noise, which
may affect our synthesizing performance.
Acknowledgement. This work was supported in part by NSF 1741472, 1813709,
and 1909912. The article solely reflects the opinions and conclusions of its authors but
not the funding agents.
16 L. Chen, et al.
References
1. Afouras, T., Chung, J.S., Zisserman, A.: Lrs3-ted: a large-scale dataset for visual
speech recognition. In: arXiv preprint arXiv:1809.00496 (2018)
2. Bregler, C., Covell, M., Slaney, M.: Video rewrite: Driving visual speech with audio.
In: Proceedings of the 24th annual conference on Computer graphics and interactive
techniques. pp. 353–360 (1997)
3. Cao, H., Cooper, D.G., Keutmann, M.K., Gur, R.C., Nenkova, A., Verma, R.:
Crema-d: Crowd-sourced emotional multimodal actors dataset. IEEE transactions
on affective computing 5(4), 377–390 (2014)
4. Cassell, J., McNeill, D., McCullough, K.E.: Speech-gesture mismatches: Evidence
for one underlying representation of linguistic and nonlinguistic information. Prag-
matics & cognition 7(1), 1–34 (1999)
5. Chang, Y.J., Ezzat, T.: Transferable videorealistic speech animation. In: Proceed-
ings of the 2005 ACM SIGGRAPH/Eurographics symposium on Computer ani-
mation. pp. 143–151. ACM (2005)
6. Chen, L., Li, Z., K Maddox, R., Duan, Z., Xu, C.: Lip movements generation at a
glance. In: Proceedings of the European Conference on Computer Vision (ECCV).
pp. 520–535 (2018)
7. Chen, L., Maddox, R.K., Duan, Z., Xu, C.: Hierarchical cross-modal talking face
generation with dynamic pixel-wise loss. In: Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition. pp. 7832–7841 (2019)
8. Chung, J.S., Jamaludin, A., Zisserman, A.: You said that? In: British Machine
Vision Conference (2017)
9. Chung, J.S., Nagrani, A., Zisserman, A.: Voxceleb2: Deep speaker recognition. In:
INTERSPEECH (2018)
10. Chung, J.S., Zisserman, A.: Lip reading in the wild. In: Asian Conference on Com-
puter Vision (2016)
11. Deng, J., Guo, J., Xue, N., Zafeiriou, S.: Arcface: Additive angular margin loss
for deep face recognition. In: Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition. pp. 4690–4699 (2019)
12. Feng, Y., Wu, F., Shao, X., Wang, Y., Zhou, X.: Joint 3d face reconstruction
and dense alignment with position map regression network. In: Proceedings of the
European Conference on Computer Vision (ECCV). pp. 534–551 (2018)
13. Fried, O., Tewari, A., Zollhöfer, M., Finkelstein, A., Shechtman, E., Goldman, D.B.,
Genova, K., Jin, Z., Theobalt, C., Agrawala, M.: Text-based editing of talking-head
video. ACM Transactions on Graphics (TOG) 38(4), 1–14 (2019)
14. Garrido, P., Valgaerts, L., Sarmadi, H., Steiner, I., Varanasi, K., Perez, P.,
Theobalt, C.: Vdub: Modifying face video of actors for plausible visual alignment
to a dubbed audio track. In: Computer graphics forum. vol. 34, pp. 193–204. Wiley
Online Library (2015)
15. Ginosar, S., Bar, A., Kohavi, G., Chan, C., Owens, A., Malik, J.: Learning individ-
ual styles of conversational gesture. In: Computer Vision and Pattern Recognition
(CVPR). IEEE (2019)
16. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., Hochreiter, S.: Gans trained
by a two time-scale update rule converge to a local nash equilibrium. In: Advances
in Neural Information Processing Systems. pp. 6626–6637 (2017)
17. Kim, H., Garrido, P., Tewari, A., Xu, W., Thies, J., Nießner, M., Pérez, P.,
Richardt, C., Zollhöfer, M., Theobalt, C.: Deep video portraits. ACM Transac-
tions on Graphics (TOG) 37(4), 1–14 (2018)
Talking-head Generation with Rhythmic Head Motion 17
18. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
19. Liu, K., Ostermann, J.: Realistic facial expression synthesis for an image-based
talking head. In: 2011 IEEE International Conference on Multimedia and Expo.
pp. 1–6. IEEE (2011)
20. Liu, M.Y., Huang, X., Mallya, A., Karras, T., Aila, T., Lehtinen, J., Kautz, J.: Few-
shot unsupervised image-to-image translation. In: IEEE International Conference
on Computer Vision (ICCV) (2019)
21. Liu, S., Li, T., Chen, W., Li, H.: Soft rasterizer: A differentiable renderer for
image-based 3d reasoning. The IEEE International Conference on Computer Vision
(ICCV) (Oct 2019)
22. Munhall, K.G., Jones, J.A., Callan, D.E., Kuratate, T., Vatikiotis-Bateson, E.:
Visual prosody and speech intelligibility: Head movement improves auditory speech
perception. Psychological science 15(2), 133–137 (2004)
23. Park, T., Liu, M.Y., Wang, T.C., Zhu, J.Y.: Semantic image synthesis with
spatially-adaptive normalization. In: Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition. pp. 2337–2346 (2019)
24. Pumarola, A., Agudo, A., Martinez, A.M., Sanfeliu, A., Moreno-Noguer, F.: Gan-
imation: One-shot anatomically consistent facial animation. International Journal
of Computer Vision pp. 1–16 (2019)
25. Simonyan, K., Zisserman, A.: Very deep convolutional networks for large-scale im-
age recognition. In: International Conference on Learning Representations (2015)
26. Song, Y., Zhu, J., Li, D., Wang, A., Qi, H.: Talking face generation by con-
ditional recurrent adversarial network. In: Proceedings of the Twenty-Eighth
International Joint Conference on Artificial Intelligence, IJCAI-19. pp. 919–
925. International Joint Conferences on Artificial Intelligence Organization
(7 2019). https://doi.org/10.24963/ijcai.2019/129, https://doi.org/10.24963/
ijcai.2019/129
27. Suwajanakorn, S., Seitz, S.M., Kemelmacher-Shlizerman, I.: Synthesizing obama:
learning lip sync from audio. ACM Transactions on Graphics (TOG) 36(4), 95
(2017)
28. Tian, Y., Shi, J., Li, B., Duan, Z., Xu, C.: Audio-visual event localization in uncon-
strained videos. In: Proceedings of the European Conference on Computer Vision
(ECCV). pp. 247–263 (2018)
29. Vondrick, C., Pirsiavash, H., Torralba, A.: Generating videos with scene dynamics.
In: Advances In Neural Information Processing Systems. pp. 613–621 (2016)
30. Vougioukas, K., Petridis, S., Pantic, M.: Realistic speech-driven facial animation
with gans. International Journal of Computer Vision pp. 1–16 (2019)
31. Wang, T.C., Liu, M.Y., Tao, A., Liu, G., Kautz, J., Catanzaro, B.: Few-shot
video-to-video synthesis. In: Advances in Neural Information Processing Systems
(NeurIPS) (2019)
32. Wang, T.C., Liu, M.Y., Zhu, J.Y., Liu, G., Tao, A., Kautz, J., Catanzaro, B.:
Video-to-video synthesis. In: Advances in Neural Information Processing Systems
(NeurIPS) (2018)
33. Wang, T.C., Liu, M.Y., Zhu, J.Y., Tao, A., Kautz, J., Catanzaro, B.: High-
resolution image synthesis and semantic manipulation with conditional gans. In:
Proceedings of the IEEE conference on computer vision and pattern recognition.
pp. 8798–8807 (2018)
34. Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., et al.: Image quality as-
sessment: from error visibility to structural similarity. IEEE transactions on image
processing 13(4), 600–612 (2004)
https://doi.org/10.24963/ijcai.2019/129
https://doi.org/10.24963/ijcai.2019/129
https://doi.org/10.24963/ijcai.2019/129
18 L. Chen, et al.
35. Wiles, O., Sophia Koepke, A., Zisserman, A.: X2face: A network for controlling face
generation using images, audio, and pose codes. In: Proceedings of the European
Conference on Computer Vision (ECCV). pp. 670–686 (2018)
36. Yoo, S., Bahng, H., Chung, S., Lee, J., Chang, J., Choo, J.: Coloring with limited
data: Few-shot colorization via memory augmented networks. In: Proceedings of
the IEEE Conference on Computer Vision and Pattern Recognition. pp. 11283–
11292 (2019)
37. Zakharov, E., Shysheya, A., Burkov, E., Lempitsky, V.: Few-shot adversarial learn-
ing of realistic neural talking head models. In: The IEEE International Conference
on Computer Vision (ICCV) (October 2019)
38. Zhou, H., Liu, J., Liu, Z., Liu, Y., Wang, X.: Rotate-and-render: Unsupervised pho-
torealistic face rotation from single-view images. In: IEEE Conference on Computer
Vision and Pattern Recognition (CVPR) (2020)
39. Zhou, H., Liu, Y., Liu, Z., Luo, P., Wang, X.: Talking face generation by ad-
versarially disentangled audio-visual representation. In: Proceedings of the AAAI
Conference on Artificial Intelligence. vol. 33, pp. 9299–9306 (2019)
40. Zhu, X., Lei, Z., Liu, X., Shi, H., Li, S.Z.: Face alignment across large poses: A 3d
solution. In: Proceedings of the IEEE conference on computer vision and pattern
recognition. pp. 146–155 (2016)
Talking-head Generation with Rhythmic Head Motion 19
Supplemental Materials
We introduce the network details in Sec. A. In Sec. B, we show more results in qual-
itative and quantitative perspective. Note that the actual results of other comparison
methods could be better, since we replicate them by ourselves. We will update those
results once the code is publicly available.
Raw audio
Co
nv
2d
(1
,8
,3
)
Co
nv
2d
(8
,1
6,
3)
Co
nv
1d
(5
12
,2
56
,3
)
FC
Encoded audio
Head motion
Linear (128)
Linear (256)
Conv1d(512,512,3)
Conv1d(512,512,3)
mean
FC
Linear (6)
Linear (1)
Sigm
oid
Linear (6)
Fig. 12. The details of the head motion learner, which consists of a encoder f (blue
part), an extrapolator (green part), and a discriminator (yellow part).
A Network Details
A.1 Details of The Head Motion Learner (Φ)
The head motion learner Φ consists of three sub-networks: a head motion encoding
network f , a head motion extrapolation network Extrapolator, and a discriminator
D. Fig. 12 shows the detailed network structure. Specifically, we first use f to encode
the raw audio x1:τ and its paired head motion h1:τ to network weights w, which
contains weights and biases {W, b} for a linear layer. Since the audio sampling rate is
50000 and the image sampling rate is 25FPS, the size of the input xτ+1:T should be
(T − τ)× 0.04× 50000. At time step t, we use xt−3:t+4 to represent the audio signal.
So, after stacking, the size of the input to Extrapolator is (7, (T − τ)× 0.04× 50000).
In the Extrapolator, we apply two 2D convolutional layers on the inputs and then
flatten it. Then we apply a 1D temporal convolution layer to encode it to audio feature
with the size of (T − τ, 256). Then at each time step t, we forward the feature chunk
(size of 1, 256) to a linear layer, where the weights and biases {W, b} are learned from
f . Once the Extrapolator generates all the fake head motion ĥτ+1:T , we forward it
20 L. Chen, et al.
with ground truth motion hτ+1:T to discriminator D. The D calculates the mean and
standard deviation from real and fake sequences and then output a real/fake score
based on the mean and standard deviation.
A.2 Details of The Facial Expression Learner (Ψ)
FC
Co
nv
(6
4,
 2
1,
 2
)
Co
nv
(2
56
, 2
1,
 2
)
PC
A
FC
FC
PCA
R
Head motion 
removal
Raw 
audio
Reference
landmark
Conv(num of Kernel, kernel size, stride)
Conv=[1D-conv, BatchNorm, LeakyReLU]
MSE Loss
Head motion 
manipulation
Fig. 13. The details of the facial expression learner. PCAR denotes reversed PCA
operation.
The facial expression learner Ψ is a linear regression network, which takes current
audio chunk xt−3:t+4 and reference landmark PCA components ptM as input and
output current facial expression p̂t. We use 20 PCA coefficients to represent facial
expression. We list the details in Fig. 13. We directly train and optimize the model
on the PCA components output. During inference, we reconstruct the 3D landmark
points from the 20 PCA coefficients.
A.3 Details of The 3D Unprojection Network
The unprojection network receives a RGB image ytM and predict the position map im-
age. We follow the same training strategy and network structure as [12], which employs
an encoder-decoder structure to learn the transfer function. we train the unprojection
network on 300W-LP [40], since it contains face images across different angles with the
annotation of estimated 3DMM coefficients, from which the 3D point cloud could be
easily generated. We calculate MSE loss between the predicted position map and the
ground truth position map. For training details, please refer to [12].
B More Results
B.1 Controllable Videos
We show one testing results on VoxCeleb2 dataset to demonstrate our ability of gener-
ating controllable head motion and facial expression. From Fig. 16, we can find that our
model can generate controllable videos with desired head motion and facial expressions.
Talking-head Generation with Rhythmic Head Motion 21
Re
sB
(3
2,
 4
, 2
)
Re
sB
 (6
4,
 4
, 2
)
Re
sB
 (6
4,
 4
, 1
)
Re
sB
(2
56
, 4
, 2
)
Co
nv
2d
(1
6,
 4
, 1
)
Re
sB
(3
2,
 4
, 1
)
Re
sB
(1
28
, 4
, 2
)
Re
sB
(1
28
, 4
, 1
)
Re
sB
(2
56
, 4
, 1
)
Re
sB
(5
12
, 4
, 2
)
Re
sB
(5
12
, 4
, 1
)
TC
on
v(
32
, 4
, 2
)
TC
on
v 
(6
4,
 4
, 2
)
TC
on
v 
(6
4,
 4
, 1
)
TC
on
v(
25
6,
 4
, 2
)
TC
on
v(
32
, 4
, 1
)
TC
on
v(
12
8,
 4
, 2
)
TC
on
v(
12
8,
 4
, 1
)
TC
on
v(
25
6,
 4
, 1
)
TC
on
v(
51
2,
 4
, 1
)
TC
on
v(
16
, 4
, 2
)
TC
on
v(
16
, 4
, 1
)
TC
on
v(
 3
, 4
, 1
)
TC
on
v(
3,
 4
, 1
)
TC
on
v(
3,
 4
, 1
)+
si
gm
oi
d
Position map
(num of Kernel, kernel size, stride)
ResB = Residual Block
TConv=[2D-TransConv, ReLU]
Fig. 14. The details of the unprojector. We use the method proposed in [12] as the
unprojector.
Real Video
Our Results
Real Video
Our Results
Fig. 15. The testing results on President Barack Obama’s weekly address footage
dataset [27].
B.2 Test on President Barack Obama Footage Dataset
To further improve the image quality, we fine-tuned our model with K = 8 on five
videos from the President Barack Obama’s weekly address footage dataset [27] and
leave the rest videos as testing set. Fig. 15 shows two example testing results.
Table 3. Ablation studies on VoxCeleb2 dataset. Our model mentioned in this table
are trained from scratch.
Method CSIM↑ SSIM↑ FID↓
Baseline 0.19 0.67 112
Full Model 0.44 0.71 40.8
w/o 3D-Aware 0.21 0.61 109
w/o Hybrid-Attention 0.37 0. 73 57.8
w/o Non-Linear Comp. 0.40 0.69 64.5
w/o warping 0.34 0.67 78.2
22 L. Chen, et al.
B.3 Ablation Studies
We conduct ablation experiments to study the contribution of four components: 3D-
Aware, Hybrid-Attention, Non-Linear Composition, and warping. The Baseline model
is a straightforward model without any features (e.g. 3D-Aware, Hybrid-Attention).
Table. 3 shows the quantitative results of ablation studies.
B.4 Settings of User Studies
Human subjects evaluation is conducted to investigate the visual qualities of our gen-
erated results compared with Zakharov et al. [37], Wang et al. [31] and Wiles [35]. The
ground truth videos are selected from different sources: we randomly select samples
from the testing set of LRW [10], VoxCeleb2 [9] and LRS3 [1]. Three methods are
evaluated w.r.t. two different criteria: whether participants could regard the generated
videos as realistic and whether the generated talking-head videos temporally sync with
the corresponding audio. We shuffle all the sample videos and the participants are not
aware of the mapping between videos to methods. They are asked to score the videos
on a scale of 0 (worst) to 10 (best). There are overall 20 participants involved (at least
50% of them are native English speaker), and the results are averaged over persons
and videos.
Talking-head Generation with Rhythmic Head Motion 23
Original
frames
Manipulated
frames
Original
frames
Manipulated
frames
Frames with
target facial
expression
Original
frames
Manipulated
frames
Original
frames
Manipulated
frames
Frames with
target
head motion
Fig. 16. The controllable results. The videos in upper part are manipulated with target
facial expressions while keep the head motion unchanged. We show the target facial
expression in the first row. The videos in lower part are manipulated with target head
motion while keep the facial expression unchanged.
	Talking-head Generation with Rhythmic Head Motion